{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open and close files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### always use the `with` statement to open a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until Python 2.5, the usual way to open a file and write something into it was like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = open(\"say_hi.txt\", \"w\")\n",
    "print(\"hi\", file=fh)\n",
    "print(\"ho\", file=fh)\n",
    "not_allowed = 1/0     # simulate the real world: an error happens druring the write process\n",
    "\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat say_hi.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what has been written to the file? Nothing! The file is empty. This is because the content is still in a memory buffer which has not been _flushed_ to the file. We can enforce the `flush=True` by providing this attribute to the `print` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = open(\"say_hi.txt\", \"w\")\n",
    "print(\"hi\", file=fh, flush=True)\n",
    "print(\"ho\", file=fh, flush=True)\n",
    "not_allowed = 1/0     # simulate the real world: an error happens druring the write process\n",
    "\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have flushed and written the data just before the crash:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat say_hi.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**However, this is error prone, a lot to type and easy to forget.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `with` statement is a safe way to open a file and write content. If anything happens during the writing process, the memory buffer gets automatically flushed and written to the file, and the file gets closed properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"say_hi.txt\", \"w\", encoding=\"utf-8\") as file_handle_1:\n",
    "    print(\"I ❤︎ ♚ and ♛\", file=file_handle_1)\n",
    "    not_allowed = 1/0     # still creates an error, but now the content is already saved!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still receive the error, but at least our content has now reached its destiny:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat say_hi.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read from one file, write to another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `with` statement also allows to open multiple files at the same time, allowing to copy content safely. **Note:** The backslash `\\` at the end of line 1 is needed to break the statement in two separate lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"say_hi.txt\", \"r\", encoding=\"utf-8\") as file_handle_1, \\\n",
    "     open(\"say_out.txt\", \"w\", encoding=\"utf-8\") as file_handle_2:\n",
    "    \n",
    "    content = file_handle_1.read()   # read in all content\n",
    "    content = content.rstrip(\"\\n\")\n",
    "    \n",
    "    for i in range(1,11):\n",
    "        print(f\"{i}:\\t{content}\", file=file_handle_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat say_out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read line by line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a `readline()` method available which does what it says on the lid: it reads a line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"say_out.txt\", \"r\", encoding=\"utf-8\") as file_handle_2:\n",
    "    myline = file_handle_2.readline()\n",
    "    while myline:\n",
    "        print(myline, end=\"\")  # the line already contains a newline, so we set end=\"\" to avoid double newlines\n",
    "        myline = file_handle_2.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not really convenient. Why not using **a for loop** instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"say_out.txt\", \"r\", encoding=\"utf-8\") as file_handle_2:\n",
    "    for line in file_handle_2:\n",
    "        print(line, end=\"\")   # the line already contains a newline, so we set end=\"\" to avoid double newlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get all lines of a file as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for this task we could use the `readlines()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"say_out.txt\", \"r\", encoding=\"utf-8\") as file_handle_2:\n",
    "    all_lines = file_handle_2.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost. We still have the unecessary newline in every item, which we want to get rid of. And we might want to get rid of the numbers and the tabs, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"say_out.txt\", \"r\", encoding=\"utf-8\") as file_handle_2:\n",
    "    all_lines = [line.rstrip('\\n').split(\"\\t\")[1] for line in file_handle_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line above is rather compact. It contains:\n",
    "\n",
    "1. A list comprehension: `for line in file_handle_2`\n",
    "2. for every `line` we remove the newline, using `line.rstrip(\"\\n\")` method\n",
    "3. the remaining string is splitted by the tabulator character: `split(\"\\t\")`\n",
    "4. the `split` command returns a list, and because we are only interested in the second column, we add `[1]`\n",
    "\n",
    "Voilà!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world logfile parsing using regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real world is a bit more complicated and does not match the training examples. Sysadmins use `grep`, `awk` and `sed` to extract parts of a logfile and pipe the output into another. However, these one-liners become unreadable line-noise. Here is an example how you would extract information from a logfile, using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open the `sfbios.log` in the filebrowser, so you can get an idea what kind of logfile we are dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now somebody (i.e. your boss) would like to extract a list of all sip usernames. The sip usernames can be found in a structure like this:\n",
    "\n",
    "`<property name=\"uri\">sip:rolands@tdl.lv</property>`\n",
    "\n",
    "The logfile is encoded in utf-8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGEX best practises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Always use the extended regular expression syntax with `re.X`, unless the regex is really trivial.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say, the super-genius just left your team and left you a regular expression which does some very clever data extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile('^(?P<alias_alternative>(?P<requested_entity>sample|object)(\\.(?P<attribute>\\w+))?)(\\s+(?i)AS\\s+(?P<alias>\\w+))?\\s*$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyone here can explain me what this regex should do? It is _somehow_ broken!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `re.X` flag, you can use the extended syntax and comment every piece of your regular expression separately. The same regex as above is now much easier to read and comprehend, the original intention is preserved. Because you can span the regex over many lines, you also need to specify all whitespace explicitly with `\\s` or `\\s+`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile(\n",
    "    r\"\"\"^                                              # beginning of the string\n",
    "        (?P<alias_alternative>                         # use first part as alias, if no alias is defined\n",
    "          (?P<requested_entity>sample|object)          # string starts with sample or object\n",
    "          (\\.(?P<attribute>\\w+))?                      # capture an optional .attribute\n",
    "        )\n",
    "        (                                              # capture an optional alias: entity.attribute AS alias\n",
    "          \\s+(?i)AS\\s+                                 # whitespace, ignore case of 'AS', whitespace\n",
    "          (?P<alias>\\w+)                               # capture the alias\n",
    "        )?                                             # \n",
    "        \\s*                                            # ignore any trailing whitespace\n",
    "        $                                              # end of string\n",
    "    \"\"\",\n",
    "    re.X + re.I\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do not use `re.match`, always use `re.search`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This regular expression below does **not match anything**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "line = \"Cats are smarter than dogs\"\n",
    "re.match(\"dogs$\", line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but this **does**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "line = \"Cats are smarter than dogs\"\n",
    "re.search(\"dogs$\", line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why?** The difference between `re.match()` and `re.search()` is that `re.match()` behaves as if every pattern has `\\A` prepended (or `^` if you don't use multiline). Anyone accustomed to Perl, grep, or sed regular expression matching is mislead by `re.match()`.\n",
    "\n",
    "There is actually a reason why re.match exists at all: it is **speed**. When `re.search()` is used and no matching is possible, it takes a considerable amount [more time](https://stackoverflow.com/questions/29007197/why-have-re-match) than `re.match()` until the matching fails. I am inclined to say: Python has an implementation problem here. I think `re.match()` should better be *deprecated*, because it leads to unnecessary problems, despite the speed gain one might observe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make use of **named capture groups**\n",
    "\n",
    "A very common practice is to group elements in a regular expression:\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "url = '/some/url/our_first_parameter/our_second_parameter'\n",
    "match = re.search(\"^/some/url/((.*?)/(.*?))$\", url)\n",
    "match.groups()\n",
    "\n",
    "# returns\n",
    "('our_first_parameter/our_second_parameter',\n",
    " 'our_first_parameter',\n",
    " 'our_second_parameter')\n",
    "```\n",
    "\n",
    "However, this leads to the problem that the parameters fetched are positional.  If you have nested group captures, you have to count the number of the opening round brackets `(` to get the position of every parameter right. And if you decide to remove a grouping later, you will have to check every position again.\n",
    "\n",
    "\n",
    "Instead, you would rather give your groups a name so you can easily rearrange your groupings without having to worry about their positions:\n",
    "<strong>\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "url = '/some/url/our_first_parameter/our_second_parameter'\n",
    "match = re.search(r\"\"\"\n",
    "    ^                       # beginning of the string\n",
    "    /some/url/              # match the base-url\n",
    "    (\n",
    "      ?P<the_whole_thing>   # capture both parameters\n",
    "      (?P<param1>.*?)       # capture the first parameter only\n",
    "      /                     # ... followed by a /\n",
    "      (?P<param2>.*?)       # capture the second parameter only\n",
    "    )\n",
    "    $                       # end of the string\n",
    "    \"\"\", url, re.X)\n",
    "if (match):\n",
    "    print(match.groupdict())\n",
    "\n",
    "# returns\n",
    "{\n",
    "    'the_whole_thing': 'our_first_parameter/our_second_parameter',\n",
    "    'param1': 'our_first_parameter',\n",
    "    'param2': 'our_second_parameter'\n",
    "}\n",
    "```\n",
    "</strong>\n",
    "\n",
    "This leads to much more robust regular expressions, especially when we are adding new or removing existing captures.\n",
    "\n",
    "In **substitutions** or within regular expressions, named capture groups are back-referenced by\n",
    "\n",
    "```\n",
    "\\g<the_name_of_the_captured_group>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### back to the real-world problem..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now back to our real problem, we will use this extended regex syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex = re.compile(r'''\n",
    "    <property\\s name=\"uri\">  # beginning of the property element\n",
    "    (?P<sip>.*?)             # fetch content, put it named capture group «sip»\n",
    "    <\\/property>             # end of element\n",
    "    ''', re.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "regex = re.compile(r'''\n",
    "    <property\\s name=\"uri\">  # beginning of element\n",
    "    (?P<sip>.*?)             # fetch content, put it named capture group «sip»\n",
    "    <\\/property>             # end of element\n",
    "    ''', re.X)\n",
    "\n",
    "log_file_path = \"sfbios.log\"\n",
    "\n",
    "match_list = []\n",
    "with open(log_file_path, \"r\", encoding=\"utf-8\") as logfile_handle, \\\n",
    "     open(\"sip_list\", \"w\") as output_handle:\n",
    "    for line in logfile_handle:\n",
    "        match = regex.search(line)    # BE AWARE: always use re.search, NEVER re.match!\n",
    "        if match:\n",
    "            print(match.groupdict()['sip'], file=output_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat sip_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON is the de-facto standard nowadays to transfer any kind of structured data. JSON has the big advantage of being strictly defined (utf-8 encoding only), so it can be parsed easily, on all platforms, across all languages. Jupyter notebooks `*.ipynb` are purely JSON.\n",
    "\n",
    "In Python, it is very easy to open an parse JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "with open('01_interaction_with_the_file_system.ipynb', 'r', encoding='utf-8') as myfile:\n",
    "    data=myfile.read()\n",
    "\n",
    "# parse file\n",
    "document = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document['metadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data science, CSV files are the gold standard, as they can be written and read by Excel ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, when dealing with _very_ large datasets, CSV files scale better than most other formats, they can be heavily compressed and quite easy to parse. The difficulty is to determine the right delimiter and the correct encoding, it is a trial and error process. You have to **read in the whole file** before you can be sure you've got the correct encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "filepath = \"data_laboratory_result.csv\"\n",
    "\n",
    "successful_encoding = None\n",
    "for encoding in ['utf-8', 'latin-1']:\n",
    "    if successful_encoding:\n",
    "        continue\n",
    "\n",
    "    successful_delimiter = None\n",
    "    try:\n",
    "        print(f\"    Trying Encoding:   {encoding}\")\n",
    "        for delimiter in [',',';','\\t']:\n",
    "            if successful_delimiter:\n",
    "                continue\n",
    "            print(f\"    Trying delimiter:  {delimiter}\")\n",
    "\n",
    "            with open(filepath, newline='', encoding=encoding) as csvfile:\n",
    "\n",
    "                if delimiter == '\\t':\n",
    "                    used_delimiter = 'TAB'\n",
    "                else:\n",
    "                    used_delimiter = delimiter\n",
    "\n",
    "                reader = csv.DictReader(csvfile, delimiter=delimiter)   # this fails with UnicodeDecodeError if we have other decoding than utf-8\n",
    "                \n",
    "                if len(reader.fieldnames) == 1:\n",
    "                    # we probably chose a wrong delimiter \n",
    "                    next\n",
    "                else:\n",
    "                    successful_delimiter = used_delimiter\n",
    "                    \n",
    "                    print(f\"      Correct delimiter: {successful_delimiter}\")\n",
    "\n",
    "                    row_count = sum(1 for row in reader)\n",
    "                    \n",
    "                    successful_encoding = encoding\n",
    "                    print(f\"      Correct Encoding:    {successful_encoding}\")\n",
    "                    print(f\"{row_count} lines read\")\n",
    "\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"    ### encoding failed for {encoding}\")\n",
    "        next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with CSV files, using Pandas\n",
    "\n",
    "In data science and when dealing with large tabular data, `Pandas` is the most popular tool to use. If you are familiar with the R language, you will find yourself at home! Pandas comes with a `read_csv()` method, but again, you need to know in advance what exact data format you are dealing with (i.e. delimiter and encoding). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv(filepath, delimiter=';', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with XML files: BeautifulSoup and lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML files are becoming out of fashion these days, but we will have to deal with them anyway. The internal structure of any XML file is tree-like, and we need a practical way to move around the tree to extract the information we want. The most useful module to do that is **BeautifulSoup4**. It is not part of the standard library, so we have to intall it from pypi, using the `pip` utility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As BeautifulSoup is just for navigating the XML file, we need also a parser to actually transform the file into an internal data structure. The standard parser is `lxml` which we can install using `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "\n",
    "filename = \"20_Ms_215_1.xml\"\n",
    "\n",
    "parser = etree.XMLParser(dtd_validation=True, recover=True)          # set up the parser\n",
    "tree = etree.parse(filename, parser)                                 # parse the file\n",
    "unicode_string = etree.tostring(tree.getroot(), encoding='unicode')  # decode the dtd characters into unicode\n",
    "soup = BeautifulSoup(unicode_string, 'lxml-xml')                     # feed BeautifulSoup with a unicode string, use lxml-xml parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our soup ready, it is super-intuitive to navigate through the tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.find('title')\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title.parent.find('principal').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in title.children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with `yaml` files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YAML files are becoming more popular these days, as Kubernetes' helm charts are written in YAML. It is even easier to write than JSON. In Python, they are as easy to read (and write) as JSON. `pyyaml` is the most popular module these days, it is not part of the standard library yet and needs to be installed via `pip` first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(r'categories.yaml') as file:\n",
    "    documents = yaml.full_load(file)\n",
    "\n",
    "    for item, doc in documents.items():\n",
    "        print(item, \":\", doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing YAML files is similarly easy, just create a datastructure and use the `yaml.dump()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    \"sports\" : ['soccer', 'football', 'basketball', 'cricket', 'hockey', 'table tennis'],\n",
    "    \"countries\" : ['Pakistan', 'USA', 'India', 'China', 'Germany', 'France', 'Spain']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml.dump(categories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
