{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "# Dealing with special file formats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "## JSON files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "source": [
        "JSON is the de-facto standard nowadays to transfer any kind of structured data. JSON has the big advantage of being strictly defined (utf-8 encoding only), so it can be parsed easily, on all platforms, across all languages. Jupyter notebooks `*.ipynb` are purely JSON.\n",
        "\n",
        "In Python, it is very easy to open an parse JSON files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# read file\n",
        "with open('01_interaction_with_the_file_system.ipynb', 'r', encoding='utf-8') as notebook:\n",
        "    content = notebook.read()\n",
        "\n",
        "# parse file\n",
        "document = json.loads(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "document.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "document['metadata']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "## CSV files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "source": [
        "In data science, CSV files are the gold standard, as they can be written and read by Excel ;-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "notes"
        }
      },
      "source": [
        "Of course, when dealing with _very_ large datasets, CSV files scale better than most other formats, they can be heavily compressed and quite easy to parse. The difficulty is to determine the right delimiter and the correct encoding, it is a trial and error process. You have to **read in the whole file** before you can be sure you've got the correct encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "filepath = \"data/data_laboratory_result.csv\"\n",
        "\n",
        "def read_csv(path, encoding, delimiter):\n",
        "    with open(path, newline='', encoding=encoding) as csvfile:\n",
        "        # this fails with UnicodeDecodeError if we have other decoding than utf-8:\n",
        "        reader = csv.DictReader(csvfile, delimiter=delimiter)\n",
        "        return [row for row in reader]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "outputs": [],
      "source": [
        "leave_outer_loop = False\n",
        "for encoding in ['utf-8', 'latin-1']:\n",
        "    try:\n",
        "        print(f\"    Trying Encoding:   {encoding}\")\n",
        "        for delimiter in [',', ';', '\\t']:\n",
        "            print(f\"    Trying delimiter:  {delimiter}\")\n",
        "            rows = read_csv(filepath, encoding, delimiter)\n",
        "            if len(rows[0]) == 1: # we probably chose a wrong delimiter \n",
        "                continue\n",
        "            else:\n",
        "                delimiter_representation = \"TAB\" if delimiter == \"\\t\" else delimiter\n",
        "                print(f\"      ✅ delimiter='{delimiter_representation}', encoding={encoding}\")\n",
        "                print(f\"{len(rows)} lines read\")\n",
        "                leave_outer_loop = True\n",
        "                break\n",
        "        if leave_outer_loop:\n",
        "            break\n",
        "\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"    ❌ encoding failed for {encoding}\")\n",
        "        next"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## CSV and Excel files, using `pandas`\n",
        "\n",
        "In data science and when dealing with large tabular data, `Pandas` is the most popular tool to use. If you are familiar with the R language, you will find yourself at home! Pandas comes with a `read_csv()` method, but again, you need to know in advance what exact data format you are dealing with (i.e. delimiter and encoding).\n",
        "\n",
        "The result is a so called **DataFrame**, which works like Excel on steroids and integrates very well with other data science libraries, like "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "filepath = \"data/data_laboratory_result.csv\"\n",
        "df = pd.read_csv(filepath, delimiter=';', encoding='latin-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Of course, `pandas` can also read good, old Excel files, if needed. However, you need to install the `openpyxl` library to import them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df2 = pd.read_excel('data/data_laboratory_result.xlsx')\n",
        "df2.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## XML files: `BeautifulSoup` and `lxml`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "notes"
        }
      },
      "source": [
        "XML files are becoming out of fashion these days, but we will have to deal with them anyway. The internal structure of any XML file is tree-like, and we need a practical way to move around the tree to extract the information we want. The most useful module to do that is **BeautifulSoup4**. It is not part of the standard library, so we have to intall it from pypi, using the `pip` utility:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "source": [
        "As BeautifulSoup is just for navigating the XML file, we need also a parser to actually transform the file into an internal data structure. The standard parser is `lxml` which we can install using `pip`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "!pip install lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from lxml import etree\n",
        "\n",
        "filename = \"data/20_Ms_215_1.xml\"\n",
        "\n",
        "parser = etree.XMLParser(dtd_validation=True, recover=True)          # set up the parser\n",
        "tree = etree.parse(filename, parser)                                 # parse the file\n",
        "unicode_string = etree.tostring(tree.getroot(), encoding='unicode')  # decode the dtd characters into unicode\n",
        "soup = BeautifulSoup(unicode_string, 'lxml-xml')                     # feed BeautifulSoup with a unicode string, use lxml-xml parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "Once we have our soup ready, it is super-intuitive to navigate through the tree:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "title = soup.find('title')\n",
        "title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "title.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "title.parent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "title.parent.find('principal').text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "for child in title.children:\n",
        "    print(child)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## YAML files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "notes"
        }
      },
      "source": [
        "YAML files are becoming more popular these days, as Kubernetes' helm charts are written in YAML. And Ansible playbooks are written in YAML. It is even easier to write than JSON. Not to mention XML...\n",
        "\n",
        "In Python, YAML files are as easy to read (and write) as JSON. `pyyaml` is the most popular module these days, it is unfortunately not part of the standard library yet and needs to be installed via `pip` first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "!pip install pyyaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "with open('data/categories.yaml', 'r') as file_in:\n",
        "    documents = yaml.full_load(file_in)    # use yaml.safe_load(file) for untrusted yaml files\n",
        "\n",
        "    for item, doc in documents.items():\n",
        "        print(item, \":\", doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      },
      "source": [
        "Writing YAML files is similarly easy, just create a datastructure and use the `yaml.dump()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "categories = {\n",
        "    \"sports\" : ['soccer', 'football', 'basketball', 'cricket', 'hockey', 'table tennis'],\n",
        "    \"countries\" : ['Pakistan', 'USA', 'India', 'China', 'Germany', 'France', 'Spain']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "print(yaml.dump(categories))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## `toml` files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "notes"
        }
      },
      "source": [
        "`toml` files are of particular interest in the python ecosystem, as `pyproject.toml` that describes a project or a package will soon become *the standard*. The main advantages over `yaml` or `json` files for configuration files are:\n",
        "\n",
        "- very human readable\n",
        "- no indentation necessary\n",
        "- comments are allowed everywhere\n",
        "- easily convertible into `json` or `yaml`\n",
        "- will become part of the Python standard library in [python 3.11](https://docs.python.org/3.11/whatsnew/3.11.html), no installation necessary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unless you are already using Python 3.11, you need to install the library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install tomli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tomli\n",
        "with open('data/configuration.toml', mode=\"rb\") as file_in:\n",
        "    configuration = tomli.load(file_in)   \n",
        "    for item, doc in configuration.items():\n",
        "        print(item, \":\", doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataframes with polars\n",
        "\n",
        "[Polars](https://www.pola.rs) is a DataFrame library written in Rust (hence the `.rs` URL) with Python bindings. It is in many ways *much faster* than `pandas` and offers all kinds of manipulations for table-like data. Besides `.csv` files, it can read Excel, Parquet and JSON files as well as [read from external databases](https://pola-rs.github.io/polars-book/user-guide/io/database/).\n",
        "\n",
        "\n",
        "Installation is – as always – easy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install polars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "\n",
        "query = (\n",
        "    pl.scan_csv(\"data/iris.csv\")         # lazy API, catches errors before processing and recommended for large files\n",
        "    .filter(pl.col(\"sepal_length\") > 5)\n",
        "    .groupby(\"species\")\n",
        "    .agg(pl.all().sum())\n",
        ")\n",
        "\n",
        "df = query.collect()                     # query contains a LazyFrame, .collect() returns a proper DataFrame       \n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`query` shows the plan how polars is going to process the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or more formally:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query.explain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
